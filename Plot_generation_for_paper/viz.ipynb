{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db15bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class ConfusionMatrixVisualizer:\n",
    "    \"\"\"\n",
    "    Reusable class for creating publication-ready visualizations from confusion matrices.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cm, class_names, figsize=(12, 8)):\n",
    "        \"\"\"\n",
    "        Initialize the visualizer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        cm : array-like, shape (n_classes, n_classes)\n",
    "            Confusion matrix\n",
    "        class_names : list\n",
    "            List of class names\n",
    "        figsize : tuple\n",
    "            Default figure size\n",
    "        \"\"\"\n",
    "        self.cm = np.array(cm)\n",
    "        self.class_names = class_names\n",
    "        self.n_classes = len(class_names)\n",
    "        self.figsize = figsize\n",
    "        self.metrics = self._calculate_metrics()\n",
    "        \n",
    "    def _calculate_metrics(self):\n",
    "        \"\"\"Calculate precision, recall, F1-score for each class.\"\"\"\n",
    "        metrics = []\n",
    "        \n",
    "        for i in range(self.n_classes):\n",
    "            tp = self.cm[i, i]\n",
    "            fp = self.cm[:, i].sum() - tp\n",
    "            fn = self.cm[i, :].sum() - tp\n",
    "            tn = self.cm.sum() - tp - fp - fn\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            support = self.cm[i, :].sum()\n",
    "            \n",
    "            metrics.append({\n",
    "                'class': self.class_names[i],\n",
    "                'precision': precision * 100,\n",
    "                'recall': recall * 100,\n",
    "                'f1': f1 * 100,\n",
    "                'support': int(support),\n",
    "                'tp': int(tp),\n",
    "                'fp': int(fp),\n",
    "                'fn': int(fn)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(metrics)\n",
    "    \n",
    "    def plot_f1_scores(self, save_path=None, top_n=None, sort=True):\n",
    "        \"\"\"\n",
    "        Plot F1-scores for all classes as horizontal bars.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        save_path : str, optional\n",
    "            Path to save the figure\n",
    "        top_n : int, optional\n",
    "            Show only top N and bottom N classes\n",
    "        sort : bool\n",
    "            Whether to sort by F1-score\n",
    "        \"\"\"\n",
    "        df = self.metrics.copy()\n",
    "        if sort:\n",
    "            df = df.sort_values('f1', ascending=True)\n",
    "        \n",
    "        if top_n:\n",
    "            df = pd.concat([df.head(top_n), df.tail(top_n)])\n",
    "        \n",
    "        # Color coding based on F1-score\n",
    "        colors = []\n",
    "        for f1 in df['f1']:\n",
    "            if f1 >= 90:\n",
    "                colors.append('#22c55e')  # Green\n",
    "            elif f1 >= 80:\n",
    "                colors.append('#84cc16')  # Lime\n",
    "            elif f1 >= 70:\n",
    "                colors.append('#eab308')  # Yellow\n",
    "            elif f1 >= 60:\n",
    "                colors.append('#f97316')  # Orange\n",
    "            else:\n",
    "                colors.append('#ef4444')  # Red\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, max(8, len(df) * 0.3)))\n",
    "        bars = ax.barh(df['class'], df['f1'], color=colors, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        ax.set_xlabel('F1-Score (%)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('F1-Score by Class', fontsize=14, fontweight='bold', pad=20)\n",
    "        ax.set_xlim(0, 105)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (bar, f1) in enumerate(zip(bars, df['f1'])):\n",
    "            ax.text(f1 + 1, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{f1:.1f}%', va='center', fontsize=9)\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = [\n",
    "            Rectangle((0,0),1,1, facecolor='#22c55e', label='≥90% (Excellent)'),\n",
    "            Rectangle((0,0),1,1, facecolor='#84cc16', label='80-89% (Good)'),\n",
    "            Rectangle((0,0),1,1, facecolor='#eab308', label='70-79% (Fair)'),\n",
    "            Rectangle((0,0),1,1, facecolor='#f97316', label='60-69% (Poor)'),\n",
    "            Rectangle((0,0),1,1, facecolor='#ef4444', label='<60% (Very Poor)')\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='lower right', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_metrics_comparison(self, save_path=None, top_n=10):\n",
    "        \"\"\"\n",
    "        Plot precision, recall, and F1-score comparison for best and worst classes.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        save_path : str, optional\n",
    "            Path to save the figure\n",
    "        top_n : int\n",
    "            Number of top and bottom classes to show\n",
    "        \"\"\"\n",
    "        df_sorted = self.metrics.sort_values('f1', ascending=False)\n",
    "        top_classes = df_sorted.head(top_n)\n",
    "        bottom_classes = df_sorted.tail(top_n)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Top performers\n",
    "        x = np.arange(len(top_classes))\n",
    "        width = 0.25\n",
    "        \n",
    "        ax1.barh(x - width, top_classes['precision'], width, label='Precision', color='#3b82f6')\n",
    "        ax1.barh(x, top_classes['recall'], width, label='Recall', color='#10b981')\n",
    "        ax1.barh(x + width, top_classes['f1'], width, label='F1-Score', color='#8b5cf6')\n",
    "        \n",
    "        ax1.set_yticks(x)\n",
    "        ax1.set_yticklabels(top_classes['class'])\n",
    "        ax1.set_xlabel('Score (%)', fontsize=11, fontweight='bold')\n",
    "        ax1.set_title(f'Top {top_n} Best Performing Classes', fontsize=12, fontweight='bold', color='green')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_xlim(0, 105)\n",
    "        ax1.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Bottom performers\n",
    "        x = np.arange(len(bottom_classes))\n",
    "        \n",
    "        ax2.barh(x - width, bottom_classes['precision'], width, label='Precision', color='#3b82f6')\n",
    "        ax2.barh(x, bottom_classes['recall'], width, label='Recall', color='#10b981')\n",
    "        ax2.barh(x + width, bottom_classes['f1'], width, label='F1-Score', color='#8b5cf6')\n",
    "        \n",
    "        ax2.set_yticks(x)\n",
    "        ax2.set_yticklabels(bottom_classes['class'])\n",
    "        ax2.set_xlabel('Score (%)', fontsize=11, fontweight='bold')\n",
    "        ax2.set_title(f'Top {top_n} Worst Performing Classes', fontsize=12, fontweight='bold', color='red')\n",
    "        ax2.legend(loc='lower right')\n",
    "        ax2.set_xlim(0, 105)\n",
    "        ax2.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_top_confusions(self, save_path=None, top_n=10, min_count=1):\n",
    "        \"\"\"\n",
    "        Plot the most common misclassifications.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        save_path : str, optional\n",
    "            Path to save the figure\n",
    "        top_n : int\n",
    "            Number of top confusions to show\n",
    "        min_count : int\n",
    "            Minimum number of misclassifications to include\n",
    "        \"\"\"\n",
    "        # Find all misclassifications\n",
    "        confusions = []\n",
    "        for i in range(self.n_classes):\n",
    "            for j in range(self.n_classes):\n",
    "                if i != j and self.cm[i, j] > min_count:\n",
    "                    confusions.append({\n",
    "                        'true': self.class_names[i],\n",
    "                        'pred': self.class_names[j],\n",
    "                        'count': int(self.cm[i, j])\n",
    "                    })\n",
    "        \n",
    "        # Sort by count\n",
    "        confusions = sorted(confusions, key=lambda x: x['count'], reverse=True)[:top_n]\n",
    "        \n",
    "        if not confusions:\n",
    "            print(\"No significant confusions found!\")\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, max(6, len(confusions) * 0.4)))\n",
    "        \n",
    "        y_pos = np.arange(len(confusions))\n",
    "        counts = [c['count'] for c in confusions]\n",
    "        labels = [f\"{c['true']} → {c['pred']}\" for c in confusions]\n",
    "        \n",
    "        bars = ax.barh(y_pos, counts, color='#ef4444', alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(labels, fontsize=10)\n",
    "        ax.set_xlabel('Number of Misclassifications', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'Top {len(confusions)} Most Common Misclassifications', \n",
    "                    fontsize=13, fontweight='bold', pad=20)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add count labels\n",
    "        for bar, count in zip(bars, counts):\n",
    "            ax.text(count + 0.2, bar.get_y() + bar.get_height()/2, \n",
    "                   str(count), va='center', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return confusions\n",
    "    \n",
    "    def plot_classification_report_table(self, save_path=None):\n",
    "        \"\"\"\n",
    "        Create a clean table with classification metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        save_path : str, optional\n",
    "            Path to save the figure\n",
    "        \"\"\"\n",
    "        df = self.metrics.sort_values('f1', ascending=False)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, max(8, len(df) * 0.25)))\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Prepare data for table\n",
    "        table_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            table_data.append([\n",
    "                row['class'],\n",
    "                f\"{row['precision']:.1f}\",\n",
    "                f\"{row['recall']:.1f}\",\n",
    "                f\"{row['f1']:.1f}\",\n",
    "                f\"{row['support']}\"\n",
    "            ])\n",
    "        \n",
    "        # Add summary row\n",
    "        avg_precision = (df['precision'] * df['support']).sum() / df['support'].sum()\n",
    "        avg_recall = (df['recall'] * df['support']).sum() / df['support'].sum()\n",
    "        avg_f1 = (df['f1'] * df['support']).sum() / df['support'].sum()\n",
    "        total_support = df['support'].sum()\n",
    "        \n",
    "        table_data.append(['---', '---', '---', '---', '---'])\n",
    "        table_data.append([\n",
    "            'Weighted Avg',\n",
    "            f\"{avg_precision:.1f}\",\n",
    "            f\"{avg_recall:.1f}\",\n",
    "            f\"{avg_f1:.1f}\",\n",
    "            f\"{total_support}\"\n",
    "        ])\n",
    "        \n",
    "        # Create table\n",
    "        table = ax.table(cellText=table_data,\n",
    "                        colLabels=['Class', 'Precision (%)', 'Recall (%)', 'F1-Score (%)', 'Support'],\n",
    "                        cellLoc='center',\n",
    "                        loc='center',\n",
    "                        colWidths=[0.3, 0.15, 0.15, 0.15, 0.15])\n",
    "        \n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "        table.scale(1, 1.5)\n",
    "        \n",
    "        # Style header\n",
    "        for i in range(5):\n",
    "            table[(0, i)].set_facecolor('#3b82f6')\n",
    "            table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "        \n",
    "        # Color code F1-scores\n",
    "        for i in range(1, len(table_data) - 1):  # Skip header and summary rows\n",
    "            f1_val = float(table_data[i-1][3])\n",
    "            if f1_val >= 90:\n",
    "                color = '#d1fae5'\n",
    "            elif f1_val >= 80:\n",
    "                color = '#fef3c7'\n",
    "            elif f1_val >= 70:\n",
    "                color = '#fed7aa'\n",
    "            else:\n",
    "                color = '#fecaca'\n",
    "            table[(i, 3)].set_facecolor(color)\n",
    "        \n",
    "        # Style summary row\n",
    "        table[(len(table_data), 0)].set_facecolor('#e5e7eb')\n",
    "        table[(len(table_data), 0)].set_text_props(weight='bold')\n",
    "        \n",
    "        plt.title('Complete Classification Report', fontsize=14, fontweight='bold', pad=20)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_performance_heatmap(self, save_path=None):\n",
    "        \"\"\"\n",
    "        Create a heatmap showing precision, recall, and F1-score for each class.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        save_path : str, optional\n",
    "            Path to save the figure\n",
    "        \"\"\"\n",
    "        df = self.metrics.sort_values('f1', ascending=False)\n",
    "        \n",
    "        # Prepare data for heatmap\n",
    "        heatmap_data = df[['precision', 'recall', 'f1']].values.T\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(max(12, len(df) * 0.3), 4))\n",
    "        \n",
    "        im = ax.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=100)\n",
    "        \n",
    "        # Set ticks\n",
    "        ax.set_xticks(np.arange(len(df)))\n",
    "        ax.set_yticks(np.arange(3))\n",
    "        ax.set_xticklabels(df['class'], rotation=90, ha='right')\n",
    "        ax.set_yticklabels(['Precision', 'Recall', 'F1-Score'])\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Score (%)', rotation=270, labelpad=15, fontweight='bold')\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(3):\n",
    "            for j in range(len(df)):\n",
    "                text = ax.text(j, i, f'{heatmap_data[i, j]:.1f}',\n",
    "                             ha=\"center\", va=\"center\", color=\"black\", fontsize=7)\n",
    "        \n",
    "        ax.set_title('Performance Metrics Heatmap', fontsize=13, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_per_class_accuracy(self, save_path=None):\n",
    "        \"\"\"\n",
    "        Plot per-class accuracy (recall) with support shown.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        save_path : str, optional\n",
    "            Path to save the figure\n",
    "        \"\"\"\n",
    "        df = self.metrics.sort_values('recall', ascending=True)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, max(8, len(df) * 0.3)))\n",
    "        \n",
    "        bars = ax.barh(df['class'], df['recall'], color='#3b82f6', alpha=0.7)\n",
    "        \n",
    "        # Add support as text\n",
    "        for i, (bar, support) in enumerate(zip(bars, df['support'])):\n",
    "            ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \n",
    "                   f\"n={support}\", va='center', fontsize=8, style='italic')\n",
    "        \n",
    "        ax.set_xlabel('Recall / Per-Class Accuracy (%)', fontsize=11, fontweight='bold')\n",
    "        ax.set_title('Per-Class Accuracy (with Sample Count)', fontsize=13, fontweight='bold', pad=20)\n",
    "        ax.set_xlim(0, 110)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        ax.axvline(x=df['recall'].mean(), color='red', linestyle='--', \n",
    "                  label=f\"Mean: {df['recall'].mean():.1f}%\", linewidth=2)\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_all_plots(self, output_dir='./plots/'):\n",
    "        \"\"\"\n",
    "        Generate all visualizations and save them.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_dir : str\n",
    "            Directory to save all plots\n",
    "        \"\"\"\n",
    "        import os\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"Generating all visualizations...\")\n",
    "        \n",
    "        print(\"1. F1-Score plot...\")\n",
    "        self.plot_f1_scores(save_path=f'{output_dir}f1_scores.png')\n",
    "        \n",
    "        print(\"2. Metrics comparison...\")\n",
    "        self.plot_metrics_comparison(save_path=f'{output_dir}best_worst_comparison.png')\n",
    "        \n",
    "        print(\"3. Top confusions...\")\n",
    "        self.plot_top_confusions(save_path=f'{output_dir}top_confusions.png')\n",
    "        \n",
    "        print(\"4. Classification report table...\")\n",
    "        self.plot_classification_report_table(save_path=f'{output_dir}classification_report.png')\n",
    "        \n",
    "        print(\"5. Performance heatmap...\")\n",
    "        self.plot_performance_heatmap(save_path=f'{output_dir}performance_heatmap.png')\n",
    "        \n",
    "        print(\"6. Per-class accuracy...\")\n",
    "        self.plot_per_class_accuracy(save_path=f'{output_dir}per_class_accuracy.png')\n",
    "        \n",
    "        print(f\"\\nAll plots saved to {output_dir}\")\n",
    "        \n",
    "        # Save metrics to CSV\n",
    "        self.metrics.to_csv(f'{output_dir}classification_metrics.csv', index=False)\n",
    "        print(f\"Metrics saved to {output_dir}classification_metrics.csv\")\n",
    "\n",
    "\n",
    "# =======================\n",
    "# EXAMPLE USAGE\n",
    "# =======================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # ========================================\n",
    "    # CHANGE THIS SECTION FOR YOUR CSV FILE\n",
    "    # ========================================\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    # OPTION 1: If your CSV has class names as the first column and first row\n",
    "    # (like the Bangladeshi food data you shared)\n",
    "    df = pd.read_csv('ClipConfusionMatrix1.csv', index_col=0)\n",
    "    cm = df.values\n",
    "    class_names = df.columns.tolist()\n",
    "    \n",
    "    # OPTION 2: If your CSV has no headers (just numbers)\n",
    "    # df = pd.read_csv('your_confusion_matrix.csv', header=None)\n",
    "    # cm = df.values\n",
    "    # class_names = ['Class1', 'Class2', 'Class3', ...]  # Manually specify class names\n",
    "    \n",
    "    # OPTION 3: If you want to clean the class names (remove extra characters)\n",
    "    # class_names = [name.strip().replace(' -', '') for name in class_names]\n",
    "    \n",
    "    # ========================================\n",
    "    # CREATE VISUALIZER AND GENERATE PLOTS\n",
    "    # ========================================\n",
    "    \n",
    "    # Create the visualizer\n",
    "    viz = ConfusionMatrixVisualizer(cm, class_names)\n",
    "    \n",
    "    # Generate ALL plots at once and save to a folder\n",
    "    viz.generate_all_plots(output_dir='./plots/')\n",
    "    \n",
    "    # OR generate individual plots (uncomment the ones you want)\n",
    "    # viz.plot_f1_scores(save_path='f1_scores.png')\n",
    "    # viz.plot_metrics_comparison(top_n=10, save_path='comparison.png')\n",
    "    # viz.plot_top_confusions(top_n=10, save_path='confusions.png')\n",
    "    # viz.plot_classification_report_table(save_path='report.png')\n",
    "    # viz.plot_performance_heatmap(save_path='heatmap.png')\n",
    "    # viz.plot_per_class_accuracy(save_path='accuracy.png')\n",
    "    \n",
    "    \n",
    "    # ========================================\n",
    "    # DEMO EXAMPLE (comment this out when using your own data)\n",
    "    # ========================================\n",
    "    \n",
    "    # For demonstration, here's a small example\n",
    "    # class_names_demo = ['Pizza', 'Burger', 'Pasta', 'Salad', 'Soup']\n",
    "    # cm_demo = np.array([\n",
    "    #     [45, 2, 1, 0, 2],\n",
    "    #     [1, 48, 0, 1, 0],\n",
    "    #     [2, 0, 42, 3, 3],\n",
    "    #     [0, 1, 2, 46, 1],\n",
    "    #     [1, 0, 2, 2, 45]\n",
    "    # ])\n",
    "    # viz = ConfusionMatrixVisualizer(cm_demo, class_names_demo)\n",
    "    # viz.plot_f1_scores()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NBA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
